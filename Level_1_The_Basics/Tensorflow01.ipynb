{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d18855ac",
   "metadata": {},
   "source": [
    "##  Step 1: Setting Up the Environment\n",
    "\n",
    "First, we import **TensorFlow**, the core framework for our project.\n",
    "\n",
    "`import tensorflow as tf`\n",
    "\n",
    "We use the alias `tf` as a standard convention. This line gives us access to the entire deep learning ecosystem, including Layers, Models, and Optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a86438f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080ee33e",
   "metadata": {},
   "source": [
    "##  Step 2: Loading Fashion MNIST\n",
    "\n",
    "We are working with the **Fashion MNIST** dataset, a more complex alternative to the classic handwritten digits dataset.\n",
    "\n",
    "`fmist = tf.keras.datasets.fashion_mnist`\n",
    "`(training_data, training_labels), (test_data, test_labels) = fmist.load_data()`\n",
    "\n",
    "**Key Concepts:**\n",
    "1. **Automatic Split:** The `load_data()` function conveniently returns the data already split into:\n",
    "   - **Training Set:** The data the model learns from.\n",
    "   - **Test Set:** Unseen data used to evaluate the model's performance later.\n",
    "2. **Data Shape:**\n",
    "   `print(training_data.shape)` -> `(60000, 28, 28)`\n",
    "   This confirms we have **60,000 images**, where each image is a 2D matrix of **28x28 pixels**. Understanding these dimensions is crucial for defining the network's Input Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b388c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "fmist=tf.keras .datasets.fashion_mnist\n",
    "(training_data,training_labels),(test_data,test_labels)=fmist.load_data()  \n",
    "\n",
    "print(training_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d08c178",
   "metadata": {},
   "source": [
    "##  Step 3: Normalization\n",
    "\n",
    "Before feeding images into the Neural Network, we perform a simple but critical mathematical operation.\n",
    "\n",
    "`training_data = training_data / 255.0`\n",
    "`test_data = test_data / 255.0`\n",
    "\n",
    "**Why divide by 255?**\n",
    "Digital image pixel values range from **0** (black) to **255** (white). Neural Networks converge much faster and perform better when inputs are scaled to a range between **0 and 1**.\n",
    "\n",
    "**Technical Benefits:**\n",
    "1. **Faster Convergence:** Optimization algorithms (like Gradient Descent) navigate the error surface more efficiently with small, normalized numbers.\n",
    "2. **Numerical Stability:** Prevents issues like exploding gradients in the initial layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbaf0880",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=training_data/255.0\n",
    "test_data=test_data/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6474cb",
   "metadata": {},
   "source": [
    "##  Step 4: Designing the Deep Neural Network Architecture\n",
    "\n",
    "This is the core of our project. Here, we define the structure of the **Deep Neural Network (DNN)** using the Keras `Sequential` API.\n",
    "\n",
    "The architecture we are building is a **Multi-Layer Perceptron (MLP)** designed to classify images.\n",
    "\n",
    "###  Deep Dive into the Layers\n",
    "\n",
    "**1. The Entrance: `Input(shape=(28,28))`**\n",
    "* **Role:** Explicitly defines the expected input tensor.\n",
    "* **Detail:** Since we are using **Fashion MNIST**, every image is a fixed `28x28` pixel grayscale grid. Defining this upfront helps Keras catch shape errors immediately.\n",
    "\n",
    "**2. The Bridge: `Flatten()`**\n",
    "* **The Problem:** Dense (fully connected) layers expect a flat 1D list of numbers, but our images are 2D matrices (grids).\n",
    "* **The Solution:** This layer transforms the `28x28` grid into a single flat vector of **784 pixels** ($28 \\times 28 = 784$). It contains no parameters to learn; it simply reformats the data.\n",
    "\n",
    "**3. The Brain (Hidden Layers): `Dense(128)` & `Dense(64)`**\n",
    "* **Structure:** These are **Fully Connected Layers**. Every neuron in the first layer connects to every single pixel in the input.\n",
    "* **Hierarchy of Learning:** By stacking layers (reducing from 128 neurons $\\to$ 64 neurons), we force the network to distill information. The first layer might detect simple edges, while the second combines them into complex shapes (loops, corners).\n",
    "* **The Magic of `activation='relu'`:**\n",
    "    * **ReLU (Rectified Linear Unit)** turns linear math into non-linear intelligence.\n",
    "    * **Math:** $f(x) = max(0, x)$.\n",
    "    * **Why?** Without ReLU, no matter how many layers you stack, the network would act like a single Linear Regression model. ReLU allows it to learn complex, curved boundaries between classes.\n",
    "\n",
    "**4. The Decision Maker: `Dense(10)`**\n",
    "* **Structure:** The **Output Layer** with exactly **10 neurons**.\n",
    "* **Why 10?** This corresponds strictly to our 10 clothing classes (T-shirt, Trouser, Pullover, etc.).\n",
    "* **Activation `softmax`:**\n",
    "    * This is the standard for **Multi-Class Classification**.\n",
    "    * It converts the raw output scores (logits) into a **Probability Distribution** where all 10 values sum up to 1.0 (100%).\n",
    "    * The neuron with the highest probability becomes the model's final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e345dd7d",
   "metadata": {},
   "source": [
    "##  Step 5: Compilation & Training\n",
    "\n",
    "Now we configure *how* the model learns and measure its performance.\n",
    "\n",
    "`model.compile(...)`\n",
    "\n",
    "1.  **Optimizer=\"adam\":** An adaptive learning rate optimization algorithm. It is the industry standard for starting most deep learning projects because it is fast and requires little tuning.\n",
    "2.  **Loss=\"sparse_categorical_crossentropy\":**\n",
    "    - **Categorical:** Because we have a multi-class classification problem (10 classes).\n",
    "    - **Sparse:** Because our labels are integers (e.g., 0, 1, 2) rather than One-Hot Encoded vectors. This is memory efficient.\n",
    "3.  **Metrics=[\"accuracy\"]:** To monitor what percentage of images are correctly classified.\n",
    "\n",
    "**Training:**\n",
    "`model.fit(training_data, training_labels, epochs=15)`\n",
    "The `fit` method starts the training loop. Over **15 Epochs** (iterations over the entire dataset), the model adjusts its internal weights to minimize loss and maximize accuracy using the Backpropagation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a4bba85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8235 - loss: 0.4959\n",
      "Epoch 2/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8663 - loss: 0.3676\n",
      "Epoch 3/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8781 - loss: 0.3307\n",
      "Epoch 4/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8861 - loss: 0.3077\n",
      "Epoch 5/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8927 - loss: 0.2892\n",
      "Epoch 6/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8957 - loss: 0.2785\n",
      "Epoch 7/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9007 - loss: 0.2642\n",
      "Epoch 8/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9056 - loss: 0.2543\n",
      "Epoch 9/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9079 - loss: 0.2452\n",
      "Epoch 10/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9118 - loss: 0.2367\n",
      "Epoch 11/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9131 - loss: 0.2275\n",
      "Epoch 12/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9149 - loss: 0.2233\n",
      "Epoch 13/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9191 - loss: 0.2145\n",
      "Epoch 14/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9204 - loss: 0.2088\n",
      "Epoch 15/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9235 - loss: 0.2022\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1aa507e1340>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=tf.keras.models.Sequential([\n",
    "    \n",
    "    tf.keras.Input(shape=(28,28),name=\"input_layer\"),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu, name=\"hidden_layer1\"),\n",
    "    tf.keras.layers.Dense(64, activation=tf.nn.relu, name=\"hidden_layer2\"), \n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax, name=\"output_layer\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(training_data,training_labels,epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda94164",
   "metadata": {},
   "source": [
    "##  Step 6: Model Evaluation (The Reality Check)\n",
    "\n",
    "After training is complete, we must answer the most important question: **\"Did the model actually learn concepts, or did it just memorize the training data?\"**\n",
    "\n",
    "To find out, we test the model on the **Test Set**—data it has never seen before.\n",
    "\n",
    "###  Analyzing the Results:\n",
    "\n",
    "**1. The `model.evaluate()` Method**\n",
    "* This function freezes the model's weights and runs a \"forward pass\" on the test data.\n",
    "* It compares the model's predictions against the actual answers (`test_labels`) to calculate the final Loss and Accuracy.\n",
    "* **`verbose=0`**: This argument keeps the output clean by hiding the progress bar during evaluation.\n",
    "\n",
    "**2. Interpreting the Numbers**\n",
    "Looking at the output above:\n",
    "* **Training Accuracy (from Epoch 15):** `~92.35%`\n",
    "* **Test Accuracy:** `~88.67%`\n",
    "\n",
    "**3. The Conclusion: Overfitting?**\n",
    "You will notice the **Test Accuracy** is lower than the **Training Accuracy** (a gap of about 3.6%).\n",
    "* This phenomenon is called **Overfitting**.\n",
    "* It means the model got slightly \"too good\" at recognizing the specific images in the training set (memorizing noise/details) but struggled a bit more when faced with new, unseen clothes.\n",
    "* *Pro Tip:* To close this gap in future projects, we could use techniques like **Dropout Layers** or **Data Augmentation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64ab40ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3574695289134979\n",
      "Test Accuracy: 0.8867999911308289\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy=model.evaluate(test_data, test_labels, verbose=0)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
